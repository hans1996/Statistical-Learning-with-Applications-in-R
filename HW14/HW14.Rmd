---
title: "HW14 M072040019 梅瀚中"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
10.
(a)
```{r,warning=FALSE}
set.seed(1)
library(dplyr)
library(ISLR)
library(data.table)
library(leaps)
library(ggplot2)
library(gam)
library(randomForest)
dt <- data.table(College)
n <- length(dt$Outstate)
train <- sample(n, n/2)
dt.train <- College[train, ] %>% data.table()
dt.test <- College[-train, ] %>% data.table()
reg.fit <- regsubsets(Outstate ~ ., data = dt.train, nvmax = 17, method = "forward")
reg.summary <- summary(reg.fit)

ggplot(data.frame(cp =reg.summary$cp, nrVar=1:17), aes(x=nrVar, y=cp))+xlab("Number of Variables") + ylab("Cp") + geom_line()
which.min(reg.summary$cp)
ggplot(data.frame(bic =reg.summary$bic, nrVar=1:17), aes(x=nrVar, y=bic))+xlab("Number of Variables") + ylab("BIC") + geom_line()
which.min(reg.summary$bic)
ggplot(data.frame(adjr2 =reg.summary$adjr2, nrVar=1:17), aes(x=nrVar, y=adjr2))+xlab("Number of Variables") + ylab("adjr2") + geom_line()
which.max(reg.summary$adjr2)
co <- coef(reg.fit, id = 6)
names(co)
```
(b)
```{r}
gam.fit <- gam(Outstate ~ Private + s(Room.Board, df = 2) + s(PhD, df = 2) +s(perc.alumni, df = 2) + s(Expend, df = 2) + s(Grad.Rate, df = 2), data = dt.train)
par(mfrow = c(2, 3))
plot(gam.fit, se = T, col = "red")
```
(c)
```{r}
gam.pred <- predict(gam.fit, dt.test)
gam.err <- mean((dt.test$Outstate - gam.pred)^2)
gam.err
```

```{r}
lm.pred <- predict(lm(Outstate~Private+Room.Board+PhD+perc.alumni+Expend+Grad.Rate, data = dt.train), dt.test)
lm.err <- mean((dt.test$Outstate - lm.pred)^2)
lm.err
```
(d)
```{r}
summary(gam.fit)
```
expend and outstate ,phd and outstate. 都有非線性的相關
11.(a,b)
```{r}
set.seed(666)
y <- rnorm(100)
x1 <- rnorm(100)
x2 <- rnorm(100)
beta1 <- 3.2
```
(c)
```{r}
a <- y - beta1 * x1
beta2 <- lm(a ~ x2)$coef[2]
```
(d)
```{r}
a <- y - beta2 * x2
beta1 <- lm(a ~ x1)$coef[2]
```
(e)
```{r}
iter <- 10
df <- data.frame(0.0, 0.27, 0.0)
names(df) <- c('beta0', 'beta1', 'beta2')
for (i in 1:iter) {
  beta1 <- df[nrow(df), 2]
  a <- y - beta1 * x1
  beta2 <- lm(a ~ x2)$coef[2]
  a <- y - beta2 * x2
  beta1 <- lm(a ~ x1)$coef[2]
  beta0 <- lm(a ~ x1)$coef[1]
  print(beta0)
  print(beta1)
  print(beta2)
  df[nrow(df) + 1,] <- list(beta0, beta1, beta2)
}
plot(df$beta0, col = 'red', type = 'l')
lines(df$beta1, col = 'blue')
lines(df$beta2, col = 'green')
```
(f)
```{r}
plot(df$beta0, col = 'red', type = 'l')
lines(df$beta1, col = 'blue')

lines(df$beta2, col = 'green')
res <- coef(lm(y ~ x1 + x2))
abline(h = res[1], col = 'darkred', lty = 2)

abline(h = res[2], col = 'darkblue', lty = 2)
abline(h = res[3], col = 'darkgreen', lty = 2)
```

```{r}

```

8.(a)
```{r}
library(ISLR)
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
```
(b)
```{r}
library(tree)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
```
(c)
```{r}
set.seed(66)
cv.carseats <- cv.tree(tree.carseats)
cv.carseats
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
tree.min

prune.carseats <- prune.tree(tree.carseats, best = 7)
plot(prune.carseats)
text(prune.carseats, pretty = 0)

yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
```
(d)
```{r}
set.seed(2)
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
importance(bag.carseats)
```
“Price” and “ShelveLoc” are the two most important variables.

(e) 
```{r}
set.seed(2)
rf.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 3, ntree = 500, importance = TRUE)
yhat.rf <- predict(rf.carseats, newdata = Carseats.test)
mean((yhat.rf - Carseats.test$Sales)^2)
importance(rf.carseats)

```
m^2=p 這裡取3
“Price” and “ShelveLoc” are the two most important variables.

(10)
(a)
```{r}
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
```
(b)
```{r}
Hitters.train <- Hitters[1:200,]
Hitters.test <- Hitters[201:263, ]
```
(C)
```{r}
library(gbm)
set.seed(3)
pows <- seq(-10, -0.2, by = 0.1)
lambdas <- 10^pows
train.err <- rep(NA, length(lambdas))
for (i in 1:length(lambdas)) {
    boost.hitters <- gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = lambdas[i])
    pred.train <- predict(boost.hitters, Hitters.train, n.trees = 1000)
    train.err[i] <- mean((pred.train - Hitters.train$Salary)^2)
}
plot(lambdas, train.err, type = "b", xlab = "Shrinkage values", ylab = "Training MSE")
```
(d)
```{r}
set.seed(3)
test.err <- rep(NA, length(lambdas))
for (i in 1:length(lambdas)) {
    boost.hitters <- gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = lambdas[i])
    yhat <- predict(boost.hitters, Hitters.test, n.trees = 1000)
    test.err[i] <- mean((yhat - Hitters.test$Salary)^2)
}
plot(lambdas, test.err, type = "b", xlab = "Shrinkage values", ylab = "Test MSE")
min(test.err)
lambdas[which.min(test.err)]
```
minimum test MSE is 0.2556347, and is obtained for λ=0.0398.

(E)
```{r}
set.seed(3)
library(glmnet)

x <- model.matrix(Salary ~ ., data = Hitters.train)
x.test <- model.matrix(Salary ~ ., data = Hitters.test)
y <- Hitters.train$Salary
fit1 <- glmnet(x, y, alpha = 0)
pred1 <- predict(fit1, s = 0.01, newx = x.test)
mean((pred1 - Hitters.test$Salary)^2)
fit2 <- glmnet(x, y, alpha = 1)
pred2 <- predict(fit2, s = 0.01, newx = x.test)
mean((pred2 - Hitters.test$Salary)^2)
```
The test MSE for boosting is lower than for lasso regression and ridge regression.

(f)
```{r}
library(gbm)
boost.hitters <- gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian", n.trees = 5000, shrinkage = lambdas[which.min(test.err)])
summary(boost.hitters)
```
“CAtBat” is the most important variable.
(g)
```{r}
set.seed(3)
bag.hitters <- randomForest(Salary ~ ., data = Hitters.train, mtry = 19, ntree = 500)
yhat.bag <- predict(bag.hitters, newdata = Hitters.test)
mean((yhat.bag - Hitters.test$Salary)^2)
```
(11)
(a)
```{r}
set.seed(66)
train <- 1:1000
Caravan.train <- Caravan[train, ]
Caravan.test <- Caravan[-train, ]
```
(b)
```{r}
set.seed(66)
Caravan$Purchase <- ifelse(Caravan$Purchase == "Yes", 1, 0)
boost.caravan <- gbm(Purchase ~ ., data = Caravan.train, distribution = "gaussian", n.trees = 1000, shrinkage = 0.01)
summary(boost.caravan)
```
“PPERSAUT” is the  most important variables.
(c)
```{r}

probs.test <- predict(boost.caravan, Caravan.test, n.trees = 1000, type = "response")
pred.test <- ifelse(probs.test > 0.2, 1, 0)
table(Caravan.test$Purchase, pred.test)
14/(38+14)

logit.caravan <- glm(Purchase ~., data = Caravan.train, family = "binomial")
probs.test2 <- predict(logit.caravan, Caravan.test, type = "response")
pred.test2 <- ifelse(probs.test > 0.2, 1, 0)
table(Caravan.test$Purchase, pred.test2)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
